Namespace()
<TRAIN> Epoch 0 - Batch 0: loss: 0.0639225691557.
<TRAIN> Epoch 0 - Batch 1: loss: 0.0759285390377.
<TRAIN> Epoch 0 - Batch 2: loss: 0.111699186265.
<TRAIN> Epoch 0 - Batch 3: loss: 0.0781471952796.
<TRAIN> Epoch 0 - Batch 4: loss: 0.0649287253618.
<TRAIN> Epoch 0 - Batch 5: loss: 0.0693111047149.
<TRAIN> Epoch 0 - Batch 6: loss: 0.084401845932.
<TRAIN> Epoch 0 - Batch 7: loss: 0.0671629980206.
<TRAIN> Epoch 0 - Batch 8: loss: 0.0731855481863.
<TRAIN> Epoch 0 - Batch 9: loss: 0.0883531495929.
<TRAIN> Epoch 0 - Batch 10: loss: 0.0784071534872.
<TRAIN> Epoch 0 - Batch 11: loss: 0.0799804478884.
<TRAIN> Epoch 0 - Batch 12: loss: 0.078889735043.
<TRAIN> Epoch 0 - Batch 13: loss: 0.0712744742632.
<TRAIN> Epoch 0 - Batch 14: loss: 0.124028325081.
<TRAIN> Epoch 0 - Batch 15: loss: 0.0715249031782.
<TRAIN> Epoch 0 - Batch 16: loss: 0.0694156959653.
<TRAIN> Epoch 0 - Batch 17: loss: 0.0686797201633.
<TRAIN> Epoch 0 - Batch 18: loss: 0.0737947151065.
<TRAIN> Epoch 0 - Batch 19: loss: 0.204637348652.
<TRAIN> Epoch 0 - Batch 20: loss: 0.0676428154111.
<TRAIN> Epoch 0 - Batch 21: loss: 0.0773019343615.
<TRAIN> Epoch 0 - Batch 22: loss: 0.235746234655.
<TRAIN> Epoch 0 - Batch 23: loss: 0.0933245420456.
<TRAIN> Epoch 0 - Batch 24: loss: 0.0807259082794.
<TRAIN> Epoch 0 - Batch 25: loss: 0.0765477493405.
<TRAIN> Epoch 0 - Batch 26: loss: 0.819059967995.
<TRAIN> Epoch 0 - Batch 27: loss: 0.071273304522.
<TRAIN> Epoch 0 - Batch 28: loss: 0.111435130239.
<TRAIN> Epoch 0 - Batch 29: loss: 0.0694852173328.
<TRAIN> Epoch 0 - Batch 30: loss: 0.0747518837452.
<TRAIN> Epoch 0 - Batch 31: loss: 0.0692841857672.
<TRAIN> Epoch 0 - Batch 32: loss: 0.0708312466741.
<TRAIN> Epoch 0 - Batch 33: loss: 0.0914369225502.
<TRAIN> Epoch 0 - Batch 34: loss: 0.0838337391615.
<TRAIN> Epoch 0 - Batch 35: loss: 0.07504157722.
<TRAIN> Epoch 0 - Batch 36: loss: 0.0962666422129.
<TRAIN> Epoch 0 - Batch 37: loss: 0.116221390665.
<TRAIN> Epoch 0 - Batch 38: loss: 0.0768672302365.
<TRAIN> Epoch 0 - Batch 39: loss: 0.0771359652281.
<TRAIN> Epoch 0 - Batch 40: loss: 0.112031832337.
<TRAIN> Epoch 0 - Batch 41: loss: 0.0639636144042.
<TRAIN> Epoch 0 - Batch 42: loss: 0.088346876204.
<TRAIN> Epoch 0 - Batch 43: loss: 0.0768674314022.
<TRAIN> Epoch 0 - Batch 44: loss: 0.0855447202921.
<TRAIN> Epoch 0 - Batch 45: loss: 0.0714883059263.
<TRAIN> Epoch 0 - Batch 46: loss: 0.0715347528458.
<TRAIN> Epoch 0 - Batch 47: loss: 0.0739907845855.
<TRAIN> Epoch 0 - Batch 48: loss: 0.365464687347.
<TRAIN> Epoch 0 - Batch 49: loss: 0.0730285570025.
<TRAIN> Epoch 0 - Batch 50: loss: 0.0770668014884.
<TRAIN> Epoch 0 - Batch 51: loss: 0.0771290138364.
<TRAIN> Epoch 0 - Batch 52: loss: 0.0861981287599.
<TRAIN> Epoch 0 - Batch 53: loss: 0.0747339725494.
<TRAIN> Epoch 0 - Batch 54: loss: 0.0671175569296.
<TRAIN> Epoch 0 - Batch 55: loss: 0.0901786908507.
<TRAIN> Epoch 0 - Batch 56: loss: 0.0748778283596.
<TRAIN> Epoch 0 - Batch 57: loss: 0.0887372419238.
<TRAIN> Epoch 0 - Batch 58: loss: 0.081176944077.
<TRAIN> Epoch 0 - Batch 59: loss: 0.0771483033895.
<TRAIN> Epoch 0 - Batch 60: loss: 0.11214928329.
<TRAIN> Epoch 0 - Batch 61: loss: 0.0672236382961.
<TRAIN> Epoch 0 - Batch 62: loss: 0.0722279325128.
<TRAIN> Epoch 0 - Batch 63: loss: 0.079691439867.
<TRAIN> Epoch 0 - Batch 64: loss: 0.0863330587745.
<TRAIN> Epoch 0 - Batch 65: loss: 0.120043791831.
<TRAIN> Epoch 0 - Batch 66: loss: 0.364305466413.
<TRAIN> Epoch 0 - Batch 67: loss: 0.074614867568.
<TRAIN> Epoch 0 - Batch 68: loss: 0.0899083092809.
<TRAIN> Epoch 0 - Batch 69: loss: 0.0904386118054.
<TRAIN> Epoch 0 - Batch 70: loss: 0.0760971158743.
<TRAIN> Epoch 0 - Batch 71: loss: 0.167553588748.
<TRAIN> Epoch 0 - Batch 72: loss: 0.105724409223.
<TRAIN> Epoch 0 - Batch 73: loss: 0.333220422268.
<TRAIN> Epoch 0 - Batch 74: loss: 0.0664564296603.
<TRAIN> Epoch 0 - Batch 75: loss: 0.0822318196297.
<TRAIN> Epoch 0 - Batch 76: loss: 0.0908171311021.
<TRAIN> Epoch 0 - Batch 77: loss: 0.0724539384246.
<TRAIN> Epoch 0 - Batch 78: loss: 0.0740416944027.
<TRAIN> Epoch 0 - Batch 79: loss: 0.0777924656868.
<TRAIN> Epoch 0 - Batch 80: loss: 0.0761874243617.
<TRAIN> Epoch 0 - Batch 81: loss: 0.0790230259299.
<TRAIN> Epoch 0 - Batch 82: loss: 0.25785112381.
<TRAIN> Epoch 0 - Batch 83: loss: 0.312134683132.
<TRAIN> Epoch 0 - Batch 84: loss: 0.0856061130762.
<TRAIN> Epoch 0 - Batch 85: loss: 0.0635800883174.
<TRAIN> Epoch 0 - Batch 86: loss: 0.0689888671041.
<TRAIN> Epoch 0 - Batch 87: loss: 0.0946004763246.
<TRAIN> Epoch 0 - Batch 88: loss: 0.075340397656.
<TRAIN> Epoch 0 - Batch 89: loss: 0.0740195736289.
<TRAIN> Epoch 0 - Batch 90: loss: 0.0793953165412.
<TRAIN> Epoch 0 - Batch 91: loss: 0.0811672508717.
<TRAIN> Epoch 0 - Batch 92: loss: 0.0672952532768.
<TRAIN> Epoch 0 - Batch 93: loss: 0.0933951511979.
<TRAIN> Epoch 0 - Batch 94: loss: 0.0729952827096.
<TRAIN> Epoch 0 - Batch 95: loss: 0.141784384847.
<TRAIN> Epoch 0 - Batch 96: loss: 0.0739519968629.
<TRAIN> Epoch 0 - Batch 97: loss: 0.069359831512.
<TRAIN> Epoch 0 - Batch 98: loss: 0.110821530223.
<TRAIN> Epoch 0 - Batch 99: loss: 0.0815159231424.
<TRAIN> Epoch 0 - Batch 100: loss: 0.0851016715169.
<TRAIN> Epoch 0 - Batch 101: loss: 0.070319019258.
<TRAIN> Epoch 0 - Batch 102: loss: 0.0858124569058.
<TRAIN> Epoch 0 - Batch 103: loss: 0.0882635042071.
<TRAIN> Epoch 0 - Batch 104: loss: 0.0981364250183.
<TRAIN> Epoch 0 - Batch 105: loss: 0.0677554532886.
<TRAIN> Epoch 0 - Batch 106: loss: 0.0782154798508.
<TRAIN> Epoch 0 - Batch 107: loss: 0.0831484571099.
<TRAIN> Epoch 0 - Batch 108: loss: 0.0916688218713.
<TRAIN> Epoch 0 - Batch 109: loss: 0.0771224498749.
<TRAIN> Epoch 0 - Batch 110: loss: 0.0661323964596.
<TRAIN> Epoch 0 - Batch 111: loss: 0.0863378196955.
<TRAIN> Epoch 0 - Batch 112: loss: 0.0704215168953.
<TRAIN> Epoch 0 - Batch 113: loss: 0.246201485395.
<TRAIN> Epoch 0 - Batch 114: loss: 0.07471960783.
<TRAIN> Epoch 0 - Batch 115: loss: 0.0742876678705.
<TRAIN> Epoch 0 - Batch 116: loss: 0.0891544744372.
<TRAIN> Epoch 0 - Batch 117: loss: 0.0862400308251.
<TRAIN> Epoch 0 - Batch 118: loss: 0.0835996344686.
<TRAIN> Epoch 0 - Batch 119: loss: 0.0849334821105.
<TRAIN> Epoch 0 - Batch 120: loss: 0.132440045476.
<TRAIN> Epoch 0 - Batch 121: loss: 0.120663166046.
<TRAIN> Epoch 0 - Batch 122: loss: 0.0688420683146.
<TRAIN> Epoch 0 - Batch 123: loss: 0.235962182283.
<TRAIN> Epoch 0 - Batch 124: loss: 0.100475631654.
<TRAIN> Epoch 0 - Batch 125: loss: 0.0691991597414.
<TRAIN> Epoch 0 - Batch 126: loss: 0.100339464843.
<TRAIN> Epoch 0 - Batch 127: loss: 0.0695328712463.
<TRAIN> Epoch 0 - Batch 128: loss: 0.102385245264.
<TRAIN> Epoch 0 - Batch 129: loss: 0.0690339952707.
<TRAIN> Epoch 0 - Batch 130: loss: 0.0650620013475.
<TRAIN> Epoch 0 - Batch 131: loss: 0.109115466475.
<TRAIN> Epoch 0 - Batch 132: loss: 0.117272287607.
<TRAIN> Epoch 0 - Batch 133: loss: 0.0846142843366.
<TRAIN> Epoch 0 - Batch 134: loss: 0.101606845856.
<TRAIN> Epoch 0 - Batch 135: loss: 0.0640074536204.
<TRAIN> Epoch 0 - Batch 136: loss: 0.0755002647638.
<TRAIN> Epoch 0 - Batch 137: loss: 0.0762747824192.
<TRAIN> Epoch 0 - Batch 138: loss: 0.0782199576497.
<TRAIN> Epoch 0 - Batch 139: loss: 0.0745286270976.
<TRAIN> Epoch 0 - Batch 140: loss: 0.0732332393527.
<TRAIN> Epoch 0 - Batch 141: loss: 0.0666342005134.
<TRAIN> Epoch 0 - Batch 142: loss: 0.0891452431679.
<TRAIN> Epoch 0 - Batch 143: loss: 0.075018890202.
<TRAIN> Epoch 0 - Batch 144: loss: 0.0675005093217.
<TRAIN> Epoch 0 - Batch 145: loss: 0.0919401049614.
<TRAIN> Epoch 0 - Batch 146: loss: 0.0880556106567.
<TRAIN> Epoch 0 - Batch 147: loss: 0.0798004195094.
<TRAIN> Epoch 0 - Batch 148: loss: 0.463163226843.
<TRAIN> Epoch 0 - Batch 149: loss: 0.110391281545.
<TRAIN> Epoch 0 - Batch 150: loss: 0.0679541230202.
<TRAIN> Epoch 0 - Batch 151: loss: 0.109005995095.
<TRAIN> Epoch 0 - Batch 152: loss: 0.0667938813567.
